\documentclass[french]{article}
\usepackage{babel}
\usepackage[T1]{fontenc}
\usepackage[latin1]{inputenc}
\usepackage[margin=1in]{geometry}
% Equations
\usepackage{amsmath}
\mathchardef\mhyphen="2D
%
\usepackage{array, multirow, tabularx}
%
\usepackage[table]{xcolor}
%
\usepackage{calc, graphicx}
%
\usepackage[section]{placeins}
%
\usepackage{subfig}
%
\usepackage{enumitem}

% Symbole Ensemble Reel
\usepackage{amssymb}

 
\input{symbols.tex}
\newcommand{\verts}{\vspace{10 mm}}


\begin{document}
%=========================
% OPTIONS COURANTES
\pagenumbering{gobble}
\setlength\parindent{0pt}
%-------------------------
\begin{center}
{\Large \textsc{Résumé théorique}}
\end{center}
\medskip
%==================================================
\section{Préalables}
\begin{itemize}\renewcommand{\labelitemi}{$\bullet$}
\item 
Dériver les équations normales de la régression linéaire au moyen de l'algèbre linéaire
\item[]
\item[] 
On cherche $\vctr{x}$ qui solutionne $\matx{A} \vctr{x} = \vctr{b}$. Comme l'équation n'admet pas de solution pour $\vctr{x}$, on cherche plutôt $\vctr{\hat{x}}$ qui solutionne $\matx{A}\vctr{\hat{x}} = \vctr{p}$, avec l'objectif de minimiser l'erreur $\vctr{e} = \vctr{p} - \vctr{b}$. Puisque $\vctr{p} \subset C(\matx{A})$ et que $\vctr{e}$ est minimal quand $\vctr{e}$ est perpendiculaire à $C(\matx{A})$, on conclut, par le théorème fondamental de l'algèbre linéaire, que $\vctr{e} \subset N(\matx{A}^\trsp)$ : 
\begin{align*}
\matx{A}^\trsp \vctr{e} &= \vctr{0}\\
\matx{A}^\trsp (\matx{A} \vctr{\hat{x}} - \vctr{b}) &= \vctr{0}\\
\vctr{\hat{x}} &= (\matx{A}^\trsp \matx{A})^{-1} \matx{A}^\trsp \vctr{b}
\end{align*}
%-----
\item[]
\item 
Démontrer que $\matx{A}^\trsp \matx{A}$ est inversible ssi les colonnes de $\matx{A}$ sont indépendantes
\item[]
\item[]
Si $\matx{A}\vctr{x}=\vctr{0}$, alors $\matx{A}^\trsp \matx{A}\vctr{x} = \vctr{0}$; si $\matx{A}^\trsp \matx{A} \vctr{x}= \vctr{0}$ alors $\vctr{x}^\trsp \matx{A}^\trsp \matx{A} \vctr{x}= 0$, $\|\matx{A}\vctr{x}\| = 0$, $\vctr{x} = \vctr{0}$ : $\matx{A}^\trsp \matx{A}$ et $\matx{A}$ ont le même espace nul. Si les colonnes de $\matx{A}$ sont indépendantes, $N(\matx{A})$ ne contient que le vecteur nul, $N(\matx{A}^\trsp \matx{A})$ ne contient que le vecteur nul, les colonnes de $\matx{A}^\trsp \matx{A}$ sont indépendantes et donc $\matx{A}^\trsp \matx{A}$ est inversible.
%-----
\item[]
\item 
Écrire les trois premiers termes de l'expansion en série de Taylor de $f(\vctr{x})$ où $\vctr{x} \in \mathbb{R}^n$
\item[]
\begin{align*}
f(\vctr{x}) &= f(\vctr{x}_0) + (\vctr{x}-\vctr{x_0})^\trsp \; \nabla f(\vctr{x})\evat_{\vctr{x}_0} + \frac{1}{2} (\vctr{x}-\vctr{x_0})^\trsp \; H_{f}(\vctr{x})\evat_{\vctr{x}_0}\;(\vctr{x}-\vctr{x_0}) + ...\\
\nabla f(\vctr{x})\evat_{\vctr{x}_0} &: \textrm{ gradient de } f(\vctr{x}) \textrm{ évalué à } \vctr{x}_0\\
H_{f}(\vctr{x})\evat_{\vctr{x}_0} &: \textrm{ matrice hessienne de } f(\vctr{x}) \textrm{ évaluée à } \vctr{x}_0\\ 
\end{align*}
%-----
\item[]
\item Exprimer la \textit{distance de Mahalanobis de $\vctr{\mu}$ à $\vctr{x}$} en termes des valeurs propres de $\Sigma$
\item[]
\begin{align*}
\textrm{distance de Mahalanobis : } \Delta^2 &= (\vctr{x}-\vctr{\mu})^\trsp \Sigma^{-1} (\vctr{x}-\vctr{\mu})\\
\textrm{Soit } \vctr{u}_i \textrm{ tel que } \Sigma \;\vctr{u}_i &= \lambda_i \vctr{u}_i\\
\Sigma &= \sum\limits_{i=1}^D \lambda_i \vctr{u}_i \vctr{u}_i^\trsp\\
\Sigma^{-1} &= \sum\limits_{i=1}^D \frac{1}{\lambda_i} \vctr{u}_i \vctr{u}_i^\trsp\\
\textrm{Notons } y_i &= \vctr{u}_i^\trsp (\vctr{x} - \vctr{\mu})\\
&= (\vctr{x} - \vctr{\mu})^\trsp \vctr{u}_i\\
\Delta^2 &= (\vctr{x}-\vctr{\mu})^\trsp \Sigma^{-1} (\vctr{x}-\vctr{\mu})\\
&= (\vctr{x}-\vctr{\mu})^\trsp \left(\sum\limits_{i=1}^D \frac{1}{\lambda_i} \vctr{u}_i \vctr{u}_i^\trsp \right)(\vctr{x}-\vctr{\mu})\\
&= \sum\limits_{i=1}^D \frac{1}{\lambda_i}  (\vctr{x}-\vctr{\mu})^\trsp \vctr{u}_i \vctr{u}_i^\trsp (\vctr{x}-\vctr{\mu})\\
&= \sum\limits_{i=1}^D \frac{y_i^2}{\lambda_i}
\end{align*}
\end{itemize}
%==================================================
\section{Théorie de l'information}
\begin{itemize}\renewcommand{\labelitemi}{$\bullet$}
\item 
Démontrer l'inégalité de Gibbs au moyen de l'inégalité de Jensen
\item[] 
\begin{align*}
\textrm{Inégalité de Gibbs } : D_{KL}(P||Q) &\geqslant 0\\
D_{KL}(P||Q) &= \sum\limits_{x \subset \mathcal{A}_X} P(x) \log \frac{P(x)}{Q(x)}\\
&= \sum\limits_{x \subset \mathcal{A}_X} P(x) \left(-\log \frac{Q(x)}{P(x)}\right)\\
&= E\left[-\log \frac{Q(X)}{P(X)}\right]\\
-\log \textrm{ est une fonction convexe}\\
\textrm{ par l'inégalité de Jensen, } &E[f(X)] \geqslant f(E[X])  \textrm{ : }\\
D_{KL}(P||Q) &\geqslant -\log\left(E\left[\frac{Q(X)}{P(X)}\right]\right)\\
&= -\log\left(\sum\limits_{x \subset \mathcal{A}_X} P(x) \frac{Q(x)}{P(x)} \right)\\
&= -\log\left(\sum\limits_{x \subset \mathcal{A}_X} Q(x) \right) = -\log(1)\\
D_{KL}(P||Q) &\geqslant 0
\end{align*}
\end{itemize}
%==================================================
\section{Régression linéaire}
\begin{itemize}\renewcommand{\labelitemi}{$\bullet$}
\item 
Pour le modèle de la \textit{régression linéaire aux moindres carrés}, énoncer les équations pour : l'hypothèse du modèle; la fonction d'erreur; la valeur du vecteur des pondérations qui minimise la fonction d'erreur
\item[]
\begin{align*}
\textrm{modèle : }t &= \vctr{w}^\trsp \vctr{\phi}(\vctr{x})\\
\textrm{fonction d'erreur } : E(\vctr{w}) &= \frac{1}{2}\sum\limits_{n=1}^N \left(t_n - \vctr{w}^\trsp \vctr{\phi}(\vctr{x}_n)\right)^2\\
\textrm{solution : }\vctr{w} &= (\matx{\Phi}^\trsp \matx{\Phi})^{-1} \matx{\Phi}^\trsp \datat\\
\textrm{où }\Phi_n &= \vctr{\phi}(\vctr{x}_n)^\trsp\\
\end{align*}
\item 
%-----
Pour le modèle de la \textit{régression linéaire (probabiliste)}, énoncer les équations pour : les hypothèses du modèle; la fonction d'erreur; la valeur du vecteur des pondérations qui minimise la fonction d'erreur
\item[]
\begin{align*}
\textrm{modèle : }y(\vctr{x}) &= \vctr{w}^\trsp \vctr{\phi}(\vctr{x})\\
p(t \given \vctr{x}) &= \gauss(t \given y(\vctr{x}), \beta^{-1})\\
\textrm{fonction d'erreur } : E(\vctr{w}) &= \frac{1}{2}\sum\limits_{n=1}^N \left(t_n - \vctr{w}^\trsp \vctr{\phi}(\vctr{x}_n)\right)^2 \\
\textrm{solution : }\vctr{w} &= (\matx{\Phi}^\trsp \matx{\Phi})^{-1} \matx{\Phi}^\trsp \datat\\
\textrm{où }\Phi_n &= \vctr{\phi}(\vctr{x}_n)^\trsp\\
\end{align*}
%-----
\item 
Pour le modèle de la \textit{régression linéaire bayésienne}, énoncer les équations pour : les hypothèses du modèle; la distribution \textit{a posteriori} du vecteur des pondérations; la distribution prédictive de la variable cible dans le cas où $\matx{S}_0 = \alpha \matx{I}$ et $\vctr{m}_0 = \vctr{0}$
\item[]
\begin{align*}
\textrm{modèle : }y(\vctr{x}, \vctr{w}) &= \vctr{w}^\trsp \vctr{\phi}(\vctr{x})\\
p(\vctr{w}) &= \gauss(\vctr{w} \given \vctr{m}_0, \matx{S}_0)\\
p(t \given \vctr{x},\vctr{w}) &= \gauss(t \given y(\vctr{x},\vctr{w}), \beta^{-1})\\
\textrm{distribution \textit{a posteri} } : p(\vctr{w} \given \datat, \dataX) &= \gauss(\vctr{w} \given \vctr{m}_N, \matx{S}_N)\\
\textrm{où }\vctr{m}_N &= \matx{S}_N(\matx{S}_0^{-1} \vctr{m}_0 + \beta \matx{\Phi}^\trsp \datat)\\
\matx{S}_N^{-1} &= \matx{S}_0^{-1} + \beta \matx{\Phi}^\trsp \matx{\Phi}\\
\textrm{prédictive : }p(t \given \vctr{x}, \datat, \dataX) &= \gauss(t \given  m(\vctr{x}), \sigma_N^2(\vctr{x}))\\
\textrm{où }m(\vctr{x}) &= \vctr{m}_N^T \vctr{\phi}(\vctr{x})\\
\sigma_N^2(\vctr{x}) &= \frac{1}{\beta} + \vctr{\phi}(\vctr{x})^\trsp \matx{S}_N \vctr{\phi}(\vctr{x})\\
\end{align*}
%-----
\item[]
\item Établir la correspondance entre le paramètre de régularisation dans la fonction d'erreur des moindres carrés et le paramètre de précision de la distribution a priori du vecteur des pondérations
\begin{align*}
p(\vctr{w} \given \datat) &\propto \prod\limits_{n=1}^N p(t_n \given \vctr{x}_n, \vctr{w}) \; p(\vctr{w})\\
&= \prod\limits_{n=1}^N \gauss(t_n \given y_n, \beta^{-1}) \; \gauss(\vctr{w} \given \vctr{0}, \alpha^{-1}) \textrm { où } y_n = y(\vctr{x}_n, \vctr{w})\\
&= k \prod\limits_{n=1}^N \exp(-\frac{\beta}{2}(t_n - y_n)^2) \; \exp(-\frac{\alpha}{2} \vctr{w}^\trsp \vctr{w})\\
E(\vctr{w}) &= -\ln p(\vctr{w} \given \datat) = \frac{\beta}{2} \sum\limits_{n=1}^N (t_n - y_n)^2 + \frac{\alpha}{2}\vctr{w}^\trsp \vctr{w} + K\\
E(\vctr{w}) \textrm { est miminum quand  } &\frac{\beta}{2} \sum\limits_{n=1}^N (t_n - y_n)^2 + \frac{\alpha}{2}\vctr{w}^\trsp \vctr{w} 
\textrm{  est minimum }\\
\textrm{quand  } &\frac{1}{2}\sum\limits_{n=1}^N (t_n - y_n)^2 + \frac{1}{2}\frac{\alpha}{\beta}\vctr{w}^\trsp \vctr{w} \textrm{  est minimum }
\end{align*}
\item[]
Le paramètre de régularisation $\lambda$ est donc équivalent à $\frac{\alpha}{\beta}$
%-----
\item[]
\item 
Au moyen d'une notation sans omission, formuler la triple intégrale de la distribution \textit{prédictive} obtenue dans le cadre d'un traitement bayésien complet (portant sur $\vctr{w}$, $\alpha$, $\beta$) du modèle de régression linéaire
\item[]
\begin{align*}
p(t \given \vctr{x}, \datat, \dataX) = \iiint p(t \given \vctr{x},\vctr{w}, \beta)\; p(\vctr{w} \given \datat, \dataX, \alpha)\; p(\alpha, \beta \given \datat, \dataX)\; d\vctr{w}\; d\alpha\; d\beta
\end{align*}
\end{itemize}
%==================================================
\section{Réseaux neuronaux}
\begin{itemize}\renewcommand{\labelitemi}{$\bullet$}
\item 
Énoncer la relation qui lie une sortie $y_k$ au vecteur d'entrée $\vctr{x}$ (à $D$ composantes) dans un réseau neuronal à une couche cachée composée de M unités
\begin{align*}
y_k = f(a_k) = f\left(\sum\limits_{j=0}^M w_{kj}^{(2)} \sigma\left(\sum\limits_{i=0}^D w_{ji}^{(1)} x_i\right)\right)
\end{align*}
%-----
\item[]
\item 
Dans un réseau neuronal, pour les modèle de régression, de classification binaire et de classification multinomiale, énoncer : le modèle pour la variable cible; la formulation de $y_k$ d'une unité de sortie en fonction de son activation $a_k$; la fonction d'erreur
\begin{align*}
p(t \given \vec{x},\vec{w}) &= \gauss(t \given y(\vec{x},\vec{w}), \beta^{-1})\\
y(\vec{x},\vec{w}) &= f(a) = a\\
E(\vec{w}) &= \frac{\beta}{2} \sum\limits_{n=1}^N \left\{y(\vec{x}_n, \vec{w}) - t_n\right\}^2\\
p(t \given \vec{x}, \vec{w}) &= y(\vec{x},\vec{w})^t \left\{1-y(\vec{x},\vec{w})\right\}^{1-t}\\
y(\vec{x},\vec{w}) &= f(a) = \sigma(a) = \frac{1}{1+\exp(-a)}\\
E(\vec{w}) &= -\sum\limits_{n=1}^N t_n \ln \left\{y(\vec{x}_n,\vec{w})\right\} + (1-t_n)\ln\left\{1- y(\vec{x}_n,\vec{w})\right\}\\
p(\vec{t} \given \vec{x}, \vec{w}) &= \prod\limits_{k=1}^K \left\{y_k(\vec{x},\vec{w})\right\}^{t_k}\\
y_k(\vec{x},\vec{w}) &= f(a_k) = \frac{\exp(a_k)}{\sum_j \exp(a_j)}\\
E(\vec{w}) &= -\sum\limits_{n=1}^N \sum\limits_{k=1}^K t_{nk} \,\ln \left\{y_k(\vec{x}_n,\vec{w})\right\}\\
\textrm{où $t_{nk}$ est la k\textsuperscript{e} composante} &\textrm{ de la cible $\vctr{t}$ de la n\textsuperscript{e} observation}
\end{align*}
%-----
\item[]
\item 
Pour un réseau neuronal à K sorties, avec une couche cachée composée de M unités, dériver les équations de $\nabla_{\vctr{w}}E_n$ calculé par rétropropagation
\item[]
\begin{align*}
\frac{\partial E_n}{\partial w_{ji}^{(l)}} = \frac{\partial E_n}{\partial a_{j}^{(l)}} \frac{\partial a_j^{(l)}}{\partial w_{ji}^{(l)}} &= \frac{\partial E_n}{\partial a_{j}^{(l)}} z_i^{(l-1)} \textrm{ où } z_i^{(1)} = h(a_i^{(1)}), z_i^{(0)} = x_i \\
&= \delta_j^{(l)} z_i^{(l-1)} \textrm{ où } \delta_j^{(l)} \equiv \frac{\partial E_n}{\partial a_{j}^{(l)}}\\
\delta_k^{(2)} \textrm{ se calcule} &\textrm{ directement à partir de la fonction d'erreur.}\\
\textrm{Si la fonction $f$ dans } y_k = f(a_k) \textrm{ est l'inverse de} &\textrm{ la fonction de liaison canonique de la distribution de $t \given \vctr{x}$:}\\
\delta_k^{(2)} &= y_k - t_k\\
\textrm{Quelle que soit la forme de la fonction $f$ : }\delta_j^{(1)} &= \frac{\partial E_n}{\partial a_{j}^{(1)}} = \sum\limits_{k=1}^K \frac{\partial E_n}{\partial a_k^{(2)}} \frac{\partial a_k^{(2)}}{\partial a_j^{(1)}}\\
&= \sum\limits_{k=1}^K \delta_k^{(2)} \frac{\partial a_k^{(2)}}{\partial a_j^{(1)}}\\
\textrm{De la somme } a_k^{(2)} &= w_{k0}^{(2)} h(a_0^{(1)}) + w_{k1}^{(2)} h(a_1^{(1)}) + w_{k2}^{(2)} h(a_2^{(1)}) + ...\\
\textrm{un seul terme lie } a_k^{(2)} \textrm{ et } a_j^{(1)} &: w_{kj}^{(2)} h(a_j^{(1)})\\
\frac{\partial a_k^{(2)}}{\partial a_j^{(1)}} &= w_{kj}^{(2)} \frac{d h(a)}{d a}\evat_{(a_j^{(1)})}\\
\delta_j^{(1)} &= h'(a_j^{(1)}) \sum\limits_{k=1}^K w_{kj}^{(2)} \delta_k^{(2)}
\end{align*}
\end{itemize}
%==================================================
\section{Méthodes noyau}
\begin{itemize}\renewcommand{\labelitemi}{$\bullet$}
\item 
Résumer la \textit{représentation noyau} du modèle de régression linéaire des moindres carrés régularisés (Ridge) en indiquant : la définition reliant le nouveau vecteur des paramètres $\vctr{a}$ et l'ancien vecteur $\vctr{w}$; la fonction noyau; la relation entre $\vctr{w}$ et $\vctr{a}$ qui produit une valeur minimum pour la fonction d'erreur; la solution de $\vctr{a}$ qui minimise la fonction d'erreur; la solution de régression $y(\vctr{x})$ en termes de la fonction noyau et du vecteur des paramètres $\vctr{a}$
\item[]
\begin{align*}
\matx{\Phi} \; \vctr{w} &= \datat - \lambda \vctr{a}\\
k(\vctr{x}_n, \vctr{x}_m) &= \vctr{\phi}(\vctr{x}_n)^\trsp \vctr{\phi}(\vctr{x}_m)\\
\vctr{w} &= \matx{\Phi}^\trsp \; \vctr{a}\\
\vctr{a} &= (\matx{K} + \lambda \matx{I})^{-1} \datat\\
\textrm{où } \matx{K} &= \matx{\Phi}\; \matx{\Phi}^\trsp\\
&= K_{nm} = k(\vctr{x}_n, \vctr{x}_m)\\
y(\vctr{x}) &= \vctr{a}^\trsp \vctr{k}(\vctr{x})\\
\textrm{où } \vctr{k}(\vctr{x}) &= \matx\Phi \; \vctr{\phi}(\vctr{x})\\
k_n(\vctr{x}) &= k(\vctr{x}_n, \vctr{x})
\end{align*}
%-----
\item[]
\item 
Donner une formulation complète du modèle de Nadaraya-Watson
\item[]
\begin{align*}
\textrm{Au moyen d'une densité de probabibilité} &\textrm{ modélisée par un estimateur de Parzen :}\\
p(\vctr{x}, t) &= \frac{1}{N}\sum\limits_{n=1}^N f(\vctr{x} - \vctr{x}_n, t - t_n)\\
\textrm{on déduit la fonction de régression } y(\vctr{x}) &= \mathbb{E}[t \given \vctr{x}]\\
y(\vctr{x}) &= \sum\limits_{n=1}^N k(\vctr{x}, \vctr{x_n}) t_n\\
\textrm{où }k(\vctr{x}, \vctr{x_n}) &= \frac{g(\vctr{x} - \vctr{x}_n)}{\sum\limits_{m=1}^N g(\vctr{x} -\vctr{x}_m) }\\
g(\vctr{x}) &\equiv \int_{-\infty}^{\infty} f(\vctr{x}, t)\; dt\\
\end{align*}
%-----
\item[]
\item 
Résumer le modèle de la régression linéaire par \textit{processus gaussien} en indiquant : les trois hypothèses du modèle; les trois équations intermédiaires qui témoignent d'une approche par processus gaussien; la prédictive :
\item[]
\begin{align*}
\textrm{modèle : }y(\vctr{x}, \vctr{w}) &= \vctr{w}^\trsp \vctr{\phi}(\vctr{x})\\
p(\vctr{w}) &= \gauss (\vctr{w} \given \matx{0}, \alpha^{-1} \matx{I})\\
p(t \given y) &= \gauss(t \given y, \beta^{-1})\\
\textrm{approche \textit{processus gaussien} : }p(\datay) &= \gauss(\datay \given \matx{0}, \matx{K})\\
\textrm{où } \matx{K} &=  \frac{1}{\alpha}\matx{\Phi} \matx{\Phi}^\trsp\\
K_{nm} &= \frac{1}{\alpha}\vctr{\phi}(\vctr{x}_n)^\trsp \vctr{\phi}(\vctr{x}_m)\\
&= k(\vctr{x}_n, \vctr{x}_m)  \\
p(\datat \given \datay) &= \gauss(\datat \given \datay, \beta^{-1} \matx{I})\\
p(\datat) &= \gauss(\datat \given \matx{0}, \matx{C})\\
\textrm{où } \matx{C} &= \matx{K} + \beta^{-1} \matx{I}\\
C_{nm} &= k(\vctr{x}_n, \vctr{x}_m) + \beta^{-1} \delta_{nm}  \\
\textrm{ prédictive : }
p(t \given \vctr{x}, \datat, \dataX) &= \gauss(t \given m(\vctr{x}), \sigma^2(\vctr{x}))\\
\textrm{où } m(\vctr{x}) &= \vctr{k}^\trsp \matx{C}^{-1} \datat\\
\sigma^2(\vctr{x}) &= c - \vctr{k}^\trsp \matx{C}^{-1} \vctr{k}\\
k_n &= k(\vctr{x}_n, \vctr{x})\\
c &= k(\vctr{x}, \vctr{x}) + \beta^{-1}\\
\end{align*}
\end{itemize}
\end{document}